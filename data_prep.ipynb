{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    \n",
    "    training_set = None\n",
    "    \n",
    "    def __init__(self, document, dataset):\n",
    "        self.document = document\n",
    "        self.dataset = dataset\n",
    "        self.similarity = {}\n",
    "        self.count_classes= {'business': 0, 'politics': 0, 'sport': 0, 'technology': 0}\n",
    "    \n",
    "    @classmethod\n",
    "    def define_training_set(dataset):\n",
    "        training_set = dataset\n",
    "        \n",
    "    def look_up_cat(self, doc_id):\n",
    "        doc_index = self.dataset.class_df['class'].loc[self.dataset.class_df['doc_id'] == doc_id].index[0]\n",
    "        doc_class = self.dataset.class_df['class'][doc_index]\n",
    "        return doc_class\n",
    "        \n",
    "    def classify(self, weighted=False):\n",
    "        try:\n",
    "            k = int(input())\n",
    "        except ValueError:\n",
    "            print(\"Please enter an integer >= 1\")\n",
    "            self.classify()\n",
    "        if weighted:\n",
    "            #Todo: add weighted method\n",
    "            pass\n",
    "        else:\n",
    "            return self.classify_noweight(k)\n",
    "            \n",
    "    def classify_noweight(self, k):\n",
    "        #takes the k nearest neighboors\n",
    "        sorted_similarities = sorted(self.similarity.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for i in range(k):\n",
    "            curr_doc_id = sorted_similarities[i][0]\n",
    "            curr_doc_cat = self.look_up_cat(curr_doc_id)\n",
    "            self.count_classes[curr_doc_cat] += 1\n",
    "        highest = max(self.count_classes.values())\n",
    "        potential_classes = [k for k,v in self.count_classes.items() if v == highest]\n",
    "        if len(potential_classes) > 1:\n",
    "            k -= 1 # classify text using 1 less neighboors until there are either no equality\n",
    "            return self.classify(1, weighted)\n",
    "        return potential_classes[0]\n",
    "        \n",
    "    def create_similarity_dic(self):\n",
    "        self.document.create_bag_of_words(self.dataset)\n",
    "        for doc_id in self.dataset.class_df['doc_id']:\n",
    "            if doc_id == self.document.doc_id:\n",
    "                continue #ignore entry if it is the same document...\n",
    "            curr_doc = Document(doc_id)\n",
    "            curr_doc.create_bag_of_words(self.dataset)\n",
    "            curr_cos = self.calculate_cosine(curr_doc)\n",
    "            self.similarity[int(doc_id)] = curr_cos\n",
    "        return self.similarity\n",
    "    \n",
    "    def calculate_cosine(self, other_doc):\n",
    "        numerator = 0\n",
    "        for term in self.document.bag_of_words:\n",
    "            try:\n",
    "                other_occur = other_doc.bag_of_words[term]\n",
    "            except KeyError:\n",
    "                continue #skip if term not in other document\n",
    "            numerator += self.document.bag_of_words[term] * other_occur\n",
    "        denominator_1 = math.sqrt(sum(map(lambda x:x**2, other_doc.bag_of_words.values())))\n",
    "        denominator_2 = math.sqrt(sum(map(lambda x:x**2, self.document.bag_of_words)))\n",
    "        \n",
    "        return float(numerator / (denominator_1 * denominator_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    \n",
    "    def __init__(self, data_file, class_file):\n",
    "        self.df = pd.read_csv(data_file, \n",
    "                                    sep=\" \", \n",
    "                                    skiprows=2, \n",
    "                                    names=['doc_id', 'term_id', 'nb_occurences'])\n",
    "        self.class_df = pd.read_csv(class_file, \n",
    "                                      names=['doc_id', 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Document(TextData):\n",
    "    \n",
    "    def __init__(self, doc_id):\n",
    "        self.doc_id = doc_id\n",
    "        self.bag_of_words = {}\n",
    "    \n",
    "    def create_bag_of_words(self, dataset):\n",
    "        '''returns a dictionary of all (term_id, occurrences) of the terms present in the document'''\n",
    "        df = dataset.df.loc[dataset.df['doc_id'] == self.doc_id].reset_index()\n",
    "        for i in range(df.shape[0]):\n",
    "            self.bag_of_words[df['term_id'][i]] = df['nb_occurences'][i]\n",
    "        return self.bag_of_words\n",
    "    \n",
    "    def get_category(self):\n",
    "        doc_index = self.dataset.class_df['class'].loc[self.dataset.class_df['doc_id'] == doc_id].index[0]\n",
    "        doc_class = self.dataset.class_df['class'][doc_index]\n",
    "        return doc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dataset = TextData('data/news_articles.mtx', 'data/news_articles.labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_document = Document(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = TextClassifier(my_document, my_dataset)\n",
    "# Think of making part of this a global var so you don't have to take as much processing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.create_similarity_dic()\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'business'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
